# -*- coding: utf-8 -*-
"""Zindi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pmPt0tDXJwp4skiAgz_UppC5Hi2bANZX
"""

import pandas as pd

train_data = pd.read_csv('Train.csv')
test_data = pd.read_csv('Test.csv')

train_data.isnull().sum() / len(train_data)
test_data.isnull().sum() / len(test_data)

train_data['target'].value_counts(normalize=True)

train_data.head()



# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Concatenate train and test data for unified preprocessing
train_data['is_train'] = 1  # Add a flag to differentiate train data
test_data['is_train'] = 0   # Add a flag to differentiate test data
test_data['target'] = np.nan  # Add a placeholder target column for test data

combined_data = pd.concat([train_data, test_data], axis=0)

# Convert categorical columns to category dtype
combined_data['country_id'] = combined_data['country_id'].astype('category')
combined_data['loan_type'] = combined_data['loan_type'].astype('category')
combined_data['New_versus_Repeat'] = combined_data['New_versus_Repeat'].astype('category')

# Convert date columns to datetime
combined_data['disbursement_date'] = pd.to_datetime(combined_data['disbursement_date'])
combined_data['due_date'] = pd.to_datetime(combined_data['due_date'])

# Feature engineering - Extract date parts
combined_data['disbursement_year'] = combined_data['disbursement_date'].dt.year
combined_data['disbursement_month'] = combined_data['disbursement_date'].dt.month
combined_data['disbursement_dayofweek'] = combined_data['disbursement_date'].dt.dayofweek

combined_data['due_year'] = combined_data['due_date'].dt.year
combined_data['due_month'] = combined_data['due_date'].dt.month
combined_data['due_dayofweek'] = combined_data['due_date'].dt.dayofweek

# Calculate loan duration in days
combined_data['loan_duration_days'] = (combined_data['due_date'] - combined_data['disbursement_date']).dt.days

# Apply log transformation to skewed numerical columns
combined_data['Total_Amount'] = np.log1p(combined_data['Total_Amount'])
combined_data['Total_Amount_to_Repay'] = np.log1p(combined_data['Total_Amount_to_Repay'])

# One-hot encode categorical columns
combined_data = pd.get_dummies(combined_data, columns=['country_id', 'loan_type', 'New_versus_Repeat'], drop_first=True)

# Separate back into train and test sets
train_data = combined_data[combined_data['is_train'] == 1].drop(columns=['is_train'])
test_data = combined_data[combined_data['is_train'] == 0].drop(columns=['is_train', 'target'])

# Splitting the data back into train and test
train_data = combined_data[combined_data['ID'].isin(train_data['ID'].unique())]
test_data= combined_data[combined_data['ID'].isin(test_data['ID'].unique())]

# Split train data into features (X) and target (y)
X = train_data.drop(columns=['ID','customer_id', 'tbl_loan_id', 'lender_id', 'disbursement_date', 'due_date', 'target'])
y = train_data['target']

# Apply train-test split
X_train, X_valid, y_train, y_valid = train_test_split(
    X,
    y,
    stratify=y,          # Ensure class balance in splits
    test_size=0.2,       # Use 20% for validation
    shuffle=True,        # Shuffle data to improve generalization
    random_state=42      # Ensure reproducibility
)

# Prepare test features
X_test = test_data.drop(columns=['ID','customer_id', 'tbl_loan_id', 'lender_id', 'disbursement_date', 'due_date'])

# Optional: Scale the data (if necessary)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_valid_scaled = scaler.transform(X_valid)
X_test_scaled = scaler.fit_transform(X_test)



from scipy import stats
# Step 1: Calculate the Z-score for the 'income' column
z_scores = np.abs(stats.zscore(train_data['Total_Amount'].dropna())) # Drop missing values for calculation
# Step 2: Set the threshold for Z-score (commonly 3 or -3)
threshold = 3
# Step 3: Identify the outliers
outliers = np.where(z_scores > threshold)
# Display the indices of outliers
print("Outliers detected at indices:", outliers)
# Step 4: Optionally, you can view the outliers
outlier_values = train_data['Total_Amount'].iloc[outliers]
print("Outlier values:", outlier_values)

# Ensure alignment before scaling
X_train, X_valid = X_train.align(X_valid, axis=1, fill_value=0)

# Apply scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_valid_scaled = scaler.transform(X_valid)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from tensorflow.keras import Sequential, Input
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import f1_score, classification_report

# Step 1: Preprocessing
scaler = StandardScaler()

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42, sampling_strategy=0.2)  # 20% minority class
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

# Step 2: Define the model
model = Sequential([
    Input(shape=(X_train_res.shape[1],)),
    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    Dropout(0.5),
    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    Dropout(0.4),
    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    Dropout(0.3),
    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0003),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Step 3: Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# Step 4: Train the model
history = model.fit(
    X_train_res, y_train_res,
    validation_data=(X_valid_scaled, y_valid),
    epochs=100,
    batch_size=64,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# Step 5: Evaluate the model
thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]
best_f1 = 0
best_threshold = 0

for threshold in thresholds:
    y_valid_preds = (model.predict(X_valid_scaled) > threshold).astype(int)
    f1 = f1_score(y_valid, y_valid_preds)
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = threshold

print(f"Best Threshold: {best_threshold}, Best F1 Score: {best_f1}")
print(classification_report(y_valid, (model.predict(X_valid_scaled) > best_threshold).astype(int)))

from sklearn.metrics import f1_score, roc_auc_score, classification_report, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predictions
y_pred = (model.predict(X_valid_scaled) > 0.5).astype(int).flatten()
y_pred_proba = model.predict(X_valid_scaled).flatten()

# Evaluation Metrics
f1 = f1_score(y_valid, y_pred)
roc_auc = roc_auc_score(y_valid, y_pred_proba)

print(f"F1 Score: {f1:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")
print("\nClassification Report:\n", classification_report(y_valid, y_pred))

# Confusion Matrix Visualization
ConfusionMatrixDisplay.from_predictions(
    y_valid,
    y_pred,
    display_labels=[0, 1],  # Update as needed for your labels
    cmap=plt.cm.Blues
)

plt.title("Confusion Matrix")
plt.show()

# Fit the scaler on the training data
scaler.fit(X_train)

# Transform both the training and test data using the fitted scaler
X_train_scaled = scaler.transform(X_train)
test_data_scaled = scaler.transform(test_data[X_train.columns])

# Make predictions on the test dataset
test_predictions = (model.predict(test_data_scaled) > 0.5).astype(int).flatten()
test_predictions_proba = model.predict(test_data_scaled).flatten()

# Save the predictions to a CSV file
test_data['target'] = test_predictions  # Add predictions to the test_data DataFrame
submission = test_data[['ID', 'target']]  # Replace 'customer_id' with your ID column
submission.to_csv('submission.csv', index=False)

print("Predictions saved to 'submission.csv'")